{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgXhVarQ6S/LNT9wUyrRVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isralennon/MSAAI530/blob/YAS/Best_Attempt_3_to_remove_Potential_Data_Leakage_in_Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekc0767M9EGu",
        "outputId": "cedadd6d-9a81-456a-d2a8-5c25f4ff0ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-20 10:45:20--  https://archive.ics.uci.edu/static/public/780/har70.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘har70.zip’\n",
            "\n",
            "har70.zip               [      <=>           ]  42.22M  1.11MB/s    in 38s     \n",
            "\n",
            "2025-02-20 10:46:09 (1.10 MB/s) - ‘har70.zip’ saved [44267192]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/780/har70.zip\n",
        "!unzip -q har70.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "from scipy.signal import hilbert, find_peaks, welch\n",
        "from scipy import stats\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GroupKFold, train_test_split, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class HAR70AdvancedAnalysis:\n",
        "    \"\"\"Handles specialized computations like phase synchronization & gait analysis.\"\"\"\n",
        "\n",
        "    def __init__(self, fs=50.0):\n",
        "        self.fs = fs\n",
        "\n",
        "    def compute_phase_sync(self, window_data):\n",
        "        \"\"\"Compute phase synchronization using only the first 75% of the window.\"\"\"\n",
        "        valid_part = int(len(window_data) * 0.75)  # Avoid future data\n",
        "        return {\n",
        "            f'phase_sync_{axis}': np.abs(np.mean(np.exp(1j * (\n",
        "                np.angle(hilbert(window_data[f'back_{axis}'][:valid_part])) -\n",
        "                np.angle(hilbert(window_data[f'thigh_{axis}'][:valid_part]))\n",
        "            ))))\n",
        "            for axis in 'xyz'\n",
        "        }\n",
        "\n",
        "    def analyze_gait(self, window_data):\n",
        "        \"\"\"Analyze gait parameters for walking activities using only past data.\"\"\"\n",
        "        valid_part = int(len(window_data) * 0.75)  # Avoid future data\n",
        "        peaks, _ = find_peaks(window_data['back_x'].values[:valid_part], distance=20)\n",
        "        if len(peaks) > 1:\n",
        "            step_times = np.diff(peaks) / self.fs\n",
        "            return {\n",
        "                'step_time_mean': np.mean(step_times),\n",
        "                'step_time_cv': np.std(step_times) / (np.mean(step_times) + 1e-10),\n",
        "                'cadence': 60 / (np.mean(step_times) + 1e-10)\n",
        "            }\n",
        "        return {'step_time_mean': np.nan, 'step_time_cv': np.nan, 'cadence': np.nan}\n",
        "\n",
        "\n",
        "class WindowGenerator:\n",
        "    \"\"\"Handles windowing of time series data efficiently.\"\"\"\n",
        "\n",
        "    def __init__(self, window_size, overlap):\n",
        "        self.window_size = window_size\n",
        "        self.step = int(window_size * (1 - overlap))\n",
        "\n",
        "    def generate(self, data):\n",
        "        \"\"\"Generate sliding windows for each subject.\"\"\"\n",
        "        windows = []\n",
        "        for _, subject_data in data.groupby('subject_id'):\n",
        "            n_windows = (len(subject_data) - self.window_size) // self.step + 1\n",
        "            for i in range(n_windows):\n",
        "                start_idx = i * self.step\n",
        "                end_idx = start_idx + self.window_size\n",
        "                if end_idx <= len(subject_data):\n",
        "                    windows.append(subject_data.iloc[start_idx:end_idx])\n",
        "        return windows\n",
        "\n",
        "\n",
        "class HAR70Analyzer:\n",
        "    \"\"\"Main class for processing the HAR70+ dataset and training models.\"\"\"\n",
        "\n",
        "    def __init__(self, data_dir='har70plus', n_jobs=-1, window_size=256, overlap=0.5):\n",
        "        self.data_dir = data_dir\n",
        "        self.n_jobs = n_jobs\n",
        "        self.window_generator = WindowGenerator(window_size, overlap)\n",
        "        self.advanced = HAR70AdvancedAnalysis()\n",
        "        self.scaler = None\n",
        "        self.classifier = None\n",
        "        self.feature_columns = None\n",
        "        self.fs = 50\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load and validate HAR70+ dataset.\"\"\"\n",
        "        files = glob.glob(os.path.join(self.data_dir, '*.csv'))\n",
        "        all_data = [pd.read_csv(f).assign(subject_id=os.path.basename(f).split('.')[0]) for f in tqdm(files, desc='Loading data')]\n",
        "\n",
        "        data = pd.concat(all_data, ignore_index=True)\n",
        "        data.dropna(inplace=True)\n",
        "        return data\n",
        "\n",
        "    def _extract_features(self, window_data):\n",
        "        \"\"\"Extracts features from a single window while preventing data leakage.\"\"\"\n",
        "        valid_part = int(len(window_data) * 0.75)  # Ensure no future data usage\n",
        "        features = {\n",
        "            'subject_id': window_data['subject_id'].iloc[0]\n",
        "        }\n",
        "\n",
        "        for col in ['back_x', 'back_y', 'back_z', 'thigh_x', 'thigh_y', 'thigh_z']:\n",
        "            sig = window_data[col].values[:valid_part]  # Use only first 75%\n",
        "\n",
        "            # Time-domain features\n",
        "            features.update({\n",
        "                f'{col}_{stat}': func(sig)\n",
        "                for stat, func in {\n",
        "                    'mean': np.mean,\n",
        "                    'std': np.std,\n",
        "                    'rms': lambda x: np.sqrt(np.mean(x**2)),\n",
        "                    'max': lambda x: np.max(np.abs(x)),\n",
        "                    'kurtosis': stats.kurtosis,\n",
        "                    'skewness': stats.skew\n",
        "                }.items()\n",
        "            })\n",
        "\n",
        "            # Frequency-domain features\n",
        "            f, psd = welch(sig, fs=self.fs, nperseg=min(len(sig), 256))\n",
        "            total_power = np.sum(psd)\n",
        "            psd_norm = psd / (total_power + 1e-10)\n",
        "\n",
        "            features.update({\n",
        "                f'{col}_dom_freq': f[np.argmax(psd)],\n",
        "                f'{col}_spec_entropy': -np.sum(psd_norm * np.log2(psd_norm + 1e-10)),\n",
        "                f'{col}_spec_centroid': np.sum(f * psd) / (total_power + 1e-10)\n",
        "            })\n",
        "\n",
        "            features.update({\n",
        "                f'{col}_power_{band}': np.sum(psd[idx]) / (total_power + 1e-10)\n",
        "                for band, idx in {\n",
        "                    'low': f < 1.0,\n",
        "                    'mid': (f >= 1.0) & (f < 3.0),\n",
        "                    'high': f >= 3.0\n",
        "                }.items()\n",
        "            })\n",
        "\n",
        "        # Compute phase sync and gait analysis only using past data\n",
        "        features.update(self.advanced.compute_phase_sync(window_data.iloc[:valid_part]))\n",
        "        if stats.mode(window_data['label'].values[:valid_part], keepdims=False)[0] in [1, 3]:\n",
        "            features.update(self.advanced.analyze_gait(window_data.iloc[:valid_part]))\n",
        "\n",
        "        # Assign the label based on the **first 75% of the window** to prevent future influence\n",
        "        features['label'] = stats.mode(window_data['label'].values[:valid_part], keepdims=False)[0]\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_features(self, data):\n",
        "        \"\"\"Extract features using parallel processing.\"\"\"\n",
        "        windows = self.window_generator.generate(data)\n",
        "        features_list = Parallel(n_jobs=self.n_jobs)(\n",
        "            delayed(self._extract_features)(window) for window in tqdm(windows, desc='Extracting features')\n",
        "        )\n",
        "\n",
        "        features_df = pd.DataFrame(features_list)\n",
        "        self.feature_columns = [col for col in features_df.columns if col not in ['label', 'subject_id']]\n",
        "        return features_df\n",
        "\n",
        "    def train_model(self, data, cv_folds=5):\n",
        "        \"\"\"Train and evaluate classifier with proper data leakage prevention.\"\"\"\n",
        "\n",
        "        # Split by subject (NO SUBJECT OVERLAP)\n",
        "        subjects = data['subject_id'].unique()\n",
        "        train_subjects, test_subjects = train_test_split(subjects, test_size=0.2, random_state=42)\n",
        "\n",
        "        train_data = data[data['subject_id'].isin(train_subjects)]\n",
        "        test_data = data[data['subject_id'].isin(test_subjects)]\n",
        "\n",
        "        # Extract features separately\n",
        "        train_features = self.extract_features(train_data)\n",
        "        test_features = self.extract_features(test_data)\n",
        "\n",
        "        X_train = train_features[self.feature_columns]\n",
        "        y_train = train_features['label']\n",
        "        groups_train = train_features['subject_id']\n",
        "\n",
        "        X_test = test_features[self.feature_columns]\n",
        "        y_test = test_features['label']\n",
        "\n",
        "        # Scale features (NO SCALING BEFORE SPLIT)\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        # Train model using `GroupKFold`\n",
        "        self.classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=self.n_jobs)\n",
        "        cv = GroupKFold(n_splits=cv_folds)\n",
        "\n",
        "        cross_validate(self.classifier, X_train_scaled, y_train, cv=cv, groups=groups_train, n_jobs=self.n_jobs)\n",
        "\n",
        "        self.classifier.fit(X_train_scaled, y_train)\n",
        "        y_pred = self.classifier.predict(X_test_scaled)\n",
        "\n",
        "        return classification_report(y_test, y_pred, zero_division=0)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = HAR70Analyzer()\n",
        "    data = analyzer.load_data()\n",
        "    print(analyzer.train_model(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQF219JY9oV6",
        "outputId": "5ee588d9-b7aa-44b2-9f50-15896981192f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading data: 100%|██████████| 18/18 [00:10<00:00,  1.68it/s]\n",
            "Extracting features: 100%|██████████| 13649/13649 [02:59<00:00, 76.16it/s]\n",
            "Extracting features: 100%|██████████| 3978/3978 [00:52<00:00, 76.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.97      1.00      0.98      1974\n",
            "           3       0.90      0.21      0.35        89\n",
            "           4       0.75      0.60      0.67         5\n",
            "           5       0.57      0.57      0.57         7\n",
            "           6       1.00      1.00      1.00       867\n",
            "           7       0.99      1.00      0.99       702\n",
            "           8       1.00      0.97      0.99       334\n",
            "\n",
            "    accuracy                           0.98      3978\n",
            "   macro avg       0.88      0.76      0.79      3978\n",
            "weighted avg       0.98      0.98      0.97      3978\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diagnostic code from ChatGPT 4o  to identify potential data leakage:"
      ],
      "metadata": {
        "id": "iIC1PQNZCSgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Check for Label Consistency in Windows\n",
        "We verify whether stats.mode(window_data['label'].values[:valid_part]) matches the final label in the window (window_data['label'].values[-1]).\n",
        "If they differ frequently, future information might be indirectly leaking into feature extraction.\n",
        "2. Validate Welch’s PSD Calculation\n",
        "Ensure nperseg in welch() does not exceed valid_part.\n",
        "If nperseg > valid_part, then Welch’s method is using future data.\n",
        "3. Confirm Peak Detection Uses Only Past Data\n",
        "Ensure that detected peaks are only from valid_part and that no peaks from the future (beyond valid_part) influence calculations.\n"
      ],
      "metadata": {
        "id": "NuKNUNxaCGsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import welch, find_peaks\n",
        "from scipy import stats\n",
        "\n",
        "def check_label_consistency(data, window_size=256, valid_ratio=0.75):\n",
        "    \"\"\"Check if future labels are leaking into feature extraction.\"\"\"\n",
        "    valid_part = int(window_size * valid_ratio)\n",
        "    inconsistent_labels = 0\n",
        "    total_windows = 0\n",
        "\n",
        "    for _, subject_data in data.groupby('subject_id'):\n",
        "        n_windows = (len(subject_data) - window_size) // (int(window_size * (1 - 0.5))) + 1\n",
        "        for i in range(n_windows):\n",
        "            start_idx = i * int(window_size * (1 - 0.5))\n",
        "            end_idx = start_idx + window_size\n",
        "            if end_idx > len(subject_data):\n",
        "                continue\n",
        "\n",
        "            window_labels = subject_data['label'].values[start_idx:end_idx]\n",
        "            mode_label = stats.mode(window_labels[:valid_part], keepdims=False)[0]\n",
        "            future_label = window_labels[-1]\n",
        "\n",
        "            if mode_label != future_label:\n",
        "                inconsistent_labels += 1\n",
        "\n",
        "            total_windows += 1\n",
        "\n",
        "    print(f\"Label Inconsistency Rate: {inconsistent_labels / total_windows:.2%} ({inconsistent_labels}/{total_windows})\")\n",
        "\n",
        "def check_welch_nperseg(data, window_size=256, valid_ratio=0.75, fs=50):\n",
        "    \"\"\"Ensure Welch's nperseg does not exceed valid_part of the window.\"\"\"\n",
        "    valid_part = int(window_size * valid_ratio)\n",
        "    violations = 0\n",
        "    total_checks = 0\n",
        "\n",
        "    for _, subject_data in data.groupby('subject_id'):\n",
        "        n_windows = (len(subject_data) - window_size) // (int(window_size * (1 - 0.5))) + 1\n",
        "        for i in range(n_windows):\n",
        "            start_idx = i * int(window_size * (1 - 0.5))\n",
        "            end_idx = start_idx + window_size\n",
        "            if end_idx > len(subject_data):\n",
        "                continue\n",
        "\n",
        "            sig = subject_data['back_x'].values[start_idx:start_idx+valid_part]  # Only past data\n",
        "            nperseg = min(len(sig), 256)\n",
        "\n",
        "            if nperseg > valid_part:\n",
        "                violations += 1\n",
        "\n",
        "            total_checks += 1\n",
        "\n",
        "    print(f\"Welch's nperseg Violation Rate: {violations / total_checks:.2%} ({violations}/{total_checks})\")\n",
        "\n",
        "def check_peak_detection(data, window_size=256, valid_ratio=0.75, fs=50):\n",
        "    \"\"\"Ensure peak detection only uses past data.\"\"\"\n",
        "    valid_part = int(window_size * valid_ratio)\n",
        "    peak_violations = 0\n",
        "    total_checks = 0\n",
        "\n",
        "    for _, subject_data in data.groupby('subject_id'):\n",
        "        n_windows = (len(subject_data) - window_size) // (int(window_size * (1 - 0.5))) + 1\n",
        "        for i in range(n_windows):\n",
        "            start_idx = i * int(window_size * (1 - 0.5))\n",
        "            end_idx = start_idx + window_size\n",
        "            if end_idx > len(subject_data):\n",
        "                continue\n",
        "\n",
        "            sig = subject_data['back_x'].values[start_idx:start_idx+valid_part]  # Only past data\n",
        "            peaks, _ = find_peaks(sig, distance=20)\n",
        "\n",
        "            if len(peaks) > 0 and np.any(peaks >= valid_part):\n",
        "                peak_violations += 1  # Peaks detected beyond valid range\n",
        "\n",
        "            total_checks += 1\n",
        "\n",
        "    print(f\"Peak Detection Violation Rate: {peak_violations / total_checks:.2%} ({peak_violations}/{total_checks})\")\n",
        "\n",
        "# Load your dataset\n",
        "analyzer = HAR70Analyzer()\n",
        "data = analyzer.load_data()\n",
        "\n",
        "# Run the checks\n",
        "check_label_consistency(data)\n",
        "check_welch_nperseg(data)\n",
        "check_peak_detection(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P1fWA35BlR6",
        "outputId": "418ef362-e844-46a4-e528-7b0c0e0893b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading data: 100%|██████████| 18/18 [00:04<00:00,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Inconsistency Rate: 11.52% (2030/17627)\n",
            "Welch's nperseg Violation Rate: 0.00% (0/17627)\n",
            "Peak Detection Violation Rate: 0.00% (0/17627)\n"
          ]
        }
      ]
    }
  ]
}